Simple example of Co-optimized ML Manifolds - using strained 1D flames for training. You can skip Steps 1 and 2 if desired (a formatted training data is included in the Repo)

### REQUIREMENTS ###
Python 3
Cantera
Pytorch
Pandas
Numpy
Scikit-Learn
SHERPA: https://parameter-sherpa.readthedocs.io/en/latest/gettingstarted/installation.html

-------------------- Premixed Flames Case ---------------------

### STEP 1 ###
run cantera to generate training data. This script solves a bunch of 1D flames at varied
strain rate:

    python get_strained_data.py

Or to run in parallel, e.g. on an interactivee node on Eagle:

    srun -n 36 python get_strained_data.py

### Step 2 ###
Process the data to the form required by the ML scripts

    python gen_trn_tst_data.py -i "strained_flames/strained*.csv" -o data/test_strained_data -f -g POSF10325 -m random

### Step 3 ###
Train the network (run on GPU node on Eagle or change use_GPU in script to False)

    python train_models_strained_flames_raw.py

### Step 4 ###
make some plots

    python plot_strained.py



-------------------- Autoignition Case ---------------------

### Step 1 ###
Run a bunch of 0D reactor cases with different equivalence ratios (fuel/air ratios) to generate data:

    python get_autoignition_data.py

### Step 2 ###
Process the data to the form required by the ML scripts

    python gen_trn_tst_data.py -i "autoignition/a*csv" -f -o data/test_autoignition_data -m patternfile -p "50_P"  -g CH4

### Step 3 ###
Train the network (run on GPU node on Eagle or change use_GPU in script to False)

    python train_models_autoignition.py

### Step 4 ###
Plot the network outputs

    python plot_autoignition.py

### Create a table of the data for comparison to the network

    python compile_2D_table.py


-------------------- Premixed Flame Case ---------------------

The script to generate testing and training data has been modified to it
can read in chemtable files. So first, we generate a chemtable
*set ```net_info_file = None``` in this file

    python get_pmfdat_and_ctable.py

    python gen_trn_tst_data.py -i "data/prem_drm19_phi1_p1_t298_allrxn.ctb" -t -o data/test_premixed_data -m everyother -g CH4

    python train_models_premixed.py

Now rerun the flame solver to generate the initial condition file for the network model
*set ```net_info_file = cmlm_net_info.txt``` in this file

    python get_pmfdat_and_ctable.py

-------------------- Additional Notes ---------------------
### Net Array
The `train_models_autoignition_netarray.py` file has an example on how to
set up and train a NetArray model (defined in the `net_array` module). The
main differences in terms of usage from the single net case are that subnets
must be added when or just after initializing the net to define
the net array, and that the `save_torchscript` method should be
used to save a net array. The keyword arguments might also differ from
those for the other nets -- see the `__init__` method in the
`NetArray` class. Note that `save_torchscript` saves a directory rather
than a single file.

The conversion script (`convert_net_format.py`) will work on saved net array
directories. It requires the subnet file names to be sortable (with position
in sorted filename list corresponding to the net's postion in the net array).
The `NetArray` (and `MRNetArray`) classes do this by default. Note that one
needs to supply the `--array` argument to the conversion script to convert a
`NetArray` at present.

### Source Term Specific Loss Function
The module `manifold_reduction_nets` now defines a pair of loss functions
operating on species source terms and manifold source terms. The species
source term loss function does not account for biases introduced by the
scaling, so that will need to be added in. Whether the functions are
correct or not is up for debate.

The `train_models_autoignition_srcloss.py` script gives an example of
including this loss function when training the model. It only does so
for the FGM net (everything else is commented out).

### Flamesheet Case
Right now this just produces a simple net trained on a very small dataset.
It was used for testing whether the FlameSheet test would run and comparing
PyTorch vs. home-rolled net performance.
